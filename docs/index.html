<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion
  </title>
  <link rel="icon" type="image/x-icon" href="static/images/noise.svg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link href="static/css/twentytwenty-no-compass.css" rel="stylesheet" type="text/css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/video_comparison.js"></script>
</head>

<body>

<section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion</h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  Yujin Wang<sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://gnwekge78707.github.io">Jiarui Wu</a><sup>2,1*</sup>,</span>
                <span class="author-block">
                  Yichen Bian<sup>1*</sup>,</span>
                <span class="author-block">
                  Fan Zhang<sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://tianfan.info/">Tianfan Xue</a><sup>2,1</sup></span>
                </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Shanghai AI Laboratory,</span>
                    <span class="author-block"><sup>2</sup>The Chinese University of Hong Kong</span>
                  </div>
                  <br>
                    <!-- <strong>NeurIPS 2024</strong> -->
                  <div class="column has-text-centered">
                    <div class="publication-links">
                    <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code(Coming Soon)</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fa fa-database"></i>
                        </span>
                        <span>Data(Coming Soon)</span>
                      </a>
                    </span>
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                      </span> -->

                      <!-- <span class="link-block">
                        <a target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="far fa-images"></i>
                      </span>
                      <span>Data (coming within 2 weeks)</span> -->
                          <!-- </a>
                      </span>  -->
                  </div>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>

<!-- Paper abstract -->
<!-- <section class="section hero is-small"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>The generalization of learning-based high dynamic range (HDR) fusion is often limited by the availability of training data, as collecting large-scale HDR images from dynamic scenes is both costly and technically challenging. To address these challenges, we propose S2R-HDR, the first large-scale high-quality synthetic dataset for HDR fusion, with 24,000 HDR samples. Using Unreal Engine 5, we design a diverse set of realistic HDR scenes that encompass various dynamic elements, motion types, high dynamic range scenes, and lighting. Additionally, we develop an efficient rendering pipeline to generate realistic HDR images.
To further mitigate the domain gap between synthetic and real-world data, we introduce S2R-Adapter, a domain adaptation designed to bridge this gap and enhance the generalization ability of models. Experimental results on real-world datasets demonstrate that our approach achieves state-of-the-art HDR reconstruction performance. 
          </p>
        </div>
      </div>
    </div>
  </div>
<!-- End paper abstract -->

  <br>
<!-- Main idea -->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Main Idea</h2>
      <div class="content has-text-justified is-four-fifths">

        <strong style="color:#2f5faf;">The generalization of learning-based high dynamic range (HDR) fusion is often limited by the availability of training data, as collecting large-scale HDR images from dynamic scenes is both costly and technically challenging. To address these challenges: </strong>
        <ul>
          <li> 
            We propose <strong>S2R-HDR</strong>, the first large-scale high-quality synthetic dataset for HDR fusion, with 24,000 HDR samples. 
            Using Unreal Engine 5, we design a diverse set of realistic HDR scenes that encompass various dynamic elements, motion types, high dynamic range scenes, and lighting.
            <p></p>
            <figure style="text-align:center;">
              <img src="static/images/Dataset_a.png" alt="" >
              <figcaption style="font-size:0.7em; color:#555;font-style: italic; text-align: center;">
                Illustration of our S2R-HDR dataset, covering both indoor and outdoor environments under diverse lighting conditions, including daytime, dusk, and nighttime, as well as various motion types such as humans, animals, and vehicles.
              </figcaption>
            </figure>
          </li>
          <li> 
            To further mitigate the domain gap between synthetic and real-world data, we introduce <strong>S2R-Adapter</strong>, a domain adaptation designed to bridge this gap and enhance the generalization ability of models.
            <p></p>
            <figure style="text-align:center;">
              <img src="static/images/Method.jpg" alt="" >
              <figcaption style="font-size:0.7em; color:#555;font-style: italic; text-align: center;">
                Structure of S2R-Adapter and t-SNE visualization of feature representations from different branches.
              </figcaption>
            </figure>
          </li>
        </ul>
      </div> 
  </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Visual Results</h2>
      <div class="content has-text-justified is-four-fifths">
        <figure style="text-align:center;">
          <img src="static/images/Teaser-v6.png" alt="" >
          <figcaption style="font-size:0.7em; color:#555;font-style: italic; text-align: center;">
            Comparing HDR fusion models trained on our S2R-HDR dataset, with the proposed domain adapter S2R-Adapter, with the same model trained on previous SCT and Challenge123 datasets. 
            Results show our dataset and training scheme can reduce ghosting artifacts under large motion (left) and recover very high dynamic range scenes, such as direct sunlight (right).
          </figcaption>
        </figure>

        <figure style="text-align:center;">
          <img src="static/images/Main-results-with-gt-v1.png" alt="" >
          <figcaption style="font-size:0.7em; color:#555;font-style: italic; text-align: center;">
            Visual results on the SCT datasets (left) and Challenge123 datasets (right) with ground-truth training data. Our method effectively eliminates artifacts caused by motion occlusions, delivering superior visual quality.
          </figcaption>
        </figure>

        <figure style="text-align:center;">
          <img src="static/images/Main-results-without-gt-v1.png" alt="" >
          <figcaption style="font-size:0.7em; color:#555;font-style: italic; text-align: center;">
            Visual results on real-captured scenes show our solution reduces ghosting in backlit scenes (left) and recovers highlights (right).
          </figcaption>
        </figure>
      </div> 
  </div>
  </div>
</section>

  
<!-- Youtube video -->
<!-- <section class="hero is-small is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths"> -->
          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/ro_XeA75s5w?si=HAR0LF3s3h5Jk6z1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <!-- </div> -->
          <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
            <!-- Your video here -->
            <!-- <source src="./static/videos/Adpative-video.mp4"
            type="video/mp4"> -->
          <!-- </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!--BibTex citation -->
<!-- <section class="section" id="BibTeX"> -->
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2025s2r,
  title={S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion},
  author={Wang, Yujin and Wu, Jiarui and Bian, Yichen and Zhang, Fan and Xue, Tianfan},
  journal={arXiv preprint arXiv:2504.07667},
  year={2025}
}
    </code></pre>
  </div>
<!-- </section> -->
<!--End BibTex citation -->


<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="content" style="text-align: center;">
      <p>
        This website was modified from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>. Thanks for sharing this fantastic template. <br> 
        Also, this website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
    </div>
  </div>
</div>
</footer>
  <!-- End image carousel -->

  <script src="static/js/jquery.min.js"></script>
  <script src="static/js/jquery.event.move.js"></script>
  <script src="static/js/jquery.twentytwenty.js"></script>
</body>

</html>
